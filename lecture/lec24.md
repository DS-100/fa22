---
layout: page
title: Lecture 24 – Logistic Regression I
nav_exclude: true
---

# Lecture 24 – Logistic Regression I

Presented by Fernando Pérez

- [slides](https://docs.google.com/presentation/d/1NAA6UtRWEc0gol_EvAH1Rp8BdhBRJe6L20I1wwOiCJU){:target="_blank"}
- [code](https://github.com/DS-100/fa22/blob/main/lec/lec24/lec24.ipynb) ([launch](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/fa22&urlpath=lab/tree/fa22/lec/lec24/lec24.ipynb){:target="_blank"})
- [code HTML](../../resources/assets/lectures/lec24/lec24.html){:target="_blank"}
- [recording](https://bcourses.berkeley.edu/courses/1518286/external_tools/78985){:target="_blank"}

<!--
- [recording](https://youtu.be/RLQ2Qzx9f1Q){:target="_blank"}
--> 

<!--
A reminder – the right column of the table below contains _Quick Checks_. These are **not** required but suggested to help you check your understanding.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>24.1</strong> <br />Decision tree basics. Decision trees in scikit-learn.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/fz30i-PgVBc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/atBGijXTcXWtKU1o9" target="\_blank">24.1</a></td>
</tr>
<tr>
<td><strong>24.2</strong> <br />Overfitting and decision trees.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/IGzRkQkG2Vk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/Lvdnq7wFws8L5TJQ8" target="\_blank">24.2</a></td>
</tr>
<tr>
<td><strong>24.3</strong> <br />Decision tree generation. Finding the best split. Entropy and weighted entropy.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/-mekg9slre4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/ajdFcaefYerXE6sr5" target="\_blank">24.3</a></td>
</tr>
<tr>
<td><strong>24.4</strong> <br />Restricting decision tree complexity. Preventing growth and pruning. Random forests and bagging.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/e8LlOnYFXcY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/4Mt9A3r9vRHJ5MrZ7" target="\_blank">24.4</a></td>
</tr>
<tr>
<td><strong>24.5</strong> <br />Regression trees. Summary of decision trees, classification, and regression.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/bALgXcAaoDA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/q48Jmzd5o6dLsihG9" target="\_blank">24.5</a></td>
</tr>
</tbody></table>
-->
