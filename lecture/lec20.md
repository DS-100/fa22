---
layout: page
title: Lecture 20 – Classification and Logistic Regression I
nav_exclude: true
---

# Lecture 20 – Classification and Logistic Regression I

Presented by Anirudhan Badrinath and Dominic Liu

Content by Fernando Pérez, Alvin Wan, Suraj Rampure, Allen Shen, Joseph Gonzalez, Andrew Bray, Josh Hug, Lisa Yan, Ani Adhikari, and Sam Lau

- [slides](https://docs.google.com/presentation/d/1u7fR9qX9lIFyeag-rNq7uTFXLIdnjbg_gysGEZO2EsM/edit?usp=sharing){:target="_blank"}
- [code](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Fsu22&branch=main&urlpath=lab%2Ftree%2Fsu22%2Flec%2Flec20%2Flec20.ipynb){:target="_blank"}
- recording
- [supplemental video on MLE Derivation](https://youtu.be/jwXbZ6QnQmA){:target="_blank"}

<!--
A reminder – the right column of the table below contains _Quick Checks_. These are **not** required but suggested to help you check your understanding.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>20.1</strong> <br />Gradient descent in one dimension. Convexity.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/gQq93hzecHM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/m4za3Jh4ujmvVbWx5" target="\_blank">20.1</a></td>
</tr>
<tr>
<td><strong>20.2</strong> <br />Various methods of optimizing loss functions in one dimension.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/AzxMoqcyWzI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/UWr3sJARPiukg53DA" target="\_blank">20.2</a></td>
</tr>
<tr>
<td><strong>20.3</strong> <br />Gradient descent in multiple dimensions. Interpretation of gradients.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/16nIdtc5x9k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/1E4gUP3moKA8knKJ8" target="\_blank">20.3</a></td>
</tr>
<tr>
<td><strong>20.4</strong> <br />Stochastic gradient descent (SGD). Comparison between gradient descent and SGD.</td>
<td><iframe width="300" height="" src="https://youtube.com/embed/CWaZS14cdh8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></td>
<td><a href="https://forms.gle/LNxjyjkRCktkF4fR8" target="\_blank">20.4</a></td>
</tr>
</tbody></table>
-->
