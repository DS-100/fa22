{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8 â€“ Data 100, Spring 2022\n",
    "\n",
    "by Anirudhan Badrinath\n",
    "\n",
    "Content from Anirudhan Badrinath, Lisa Yan, Suraj Rampure, Ani Adhikari, and Data 8 Textbook Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Correlation\n",
    "\n",
    "Recreate the 4 correlation plots in slides (Normally this wouldn't be in the notebook, but it might be of interest to you.)\n",
    "\n",
    "Note: We use `np.corrcoef` to compute the correlation coefficient $r$, though we could also compute manually too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed recreate same random points as slides\n",
    "np.random.seed(43)\n",
    "plt.style.use('default') # revert style to default mpl\n",
    "\n",
    "def plot_and_get_corr(ax, x, y, title):\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.scatter(x, y, alpha=0.73)\n",
    "    ax.set_title(title)\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(10,10))\n",
    "\n",
    "# just noise\n",
    "x1, y1 = np.random.randn(2, 100)\n",
    "corr1 = plot_and_get_corr(axs[0, 0], x1, y1, title=\"noise\")\n",
    "\n",
    "\n",
    "# strong linear\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "y2 = x2*0.5 - 1 + np.random.randn(100)*0.3\n",
    "corr2 = plot_and_get_corr(axs[0, 1], x2, y2, title=\"strong linear\")\n",
    "\n",
    "\n",
    "# unequal spread\n",
    "x3 = np.linspace(-3, 3, 100)\n",
    "y3 = - x3/3 + np.random.randn(100)*(x3)/2.5\n",
    "corr3 = plot_and_get_corr(axs[1, 0], x3, y3, title=\"strong linear\")\n",
    "extent = axs[1, 0].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "fig.savefig('ax3_figure.png', bbox_inches=extent)\n",
    "\n",
    "\n",
    "# strong non-linear\n",
    "x4 = np.linspace(-3, 3, 100)\n",
    "y4 = 2*np.sin(x3 - 1.5) + np.random.randn(100)*0.3\n",
    "corr4 = plot_and_get_corr(axs[1, 1], x4, y4, title=\"strong non-linear\")\n",
    "# extent = axs[1, 1].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "# fig.savefig('ax4_figure.png', bbox_inches=extent)\n",
    "\n",
    "plt.show()\n",
    "print([corr1, corr2, corr3, corr4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Which $\\theta$ is best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "lw = pd.read_csv(\"little_women.csv\")\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.gca()\n",
    "sns.scatterplot(data=lw, x=\"Periods\", y=\"Characters\", ax=ax)\n",
    "\n",
    "xlims = np.array([50, 450])\n",
    "params = [[5000, 100], [50000, -50], [-4000, 150]]\n",
    "for i, (a, b) in enumerate(params):\n",
    "    ax.plot(xlims, a + b * xlims, lw=2, label=f\"{i+1}. (a: {a}, b: {b})\")\n",
    "ax.legend()\n",
    "\n",
    "# the best parameters weren't one of the choices\n",
    "ahat_true, bhat_true = 4745, 87\n",
    "fig.tight_layout()\n",
    "plt.savefig('lw_params.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's implement the tools we'll need for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    return np.mean(standard_units(x) * standard_units(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('disc_vs_score.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this data using a typical scatterplot using Seaborn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x = 'disc_score', y = 'overall_score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty hard to see what's going on here, so let's groupby each potential discussion attendance score and find the mean overall score for each, plotting that!\n",
    "\n",
    "There's a much clearer trend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('disc_score')['overall_score'].mean().plot(style = '.', markersize = 20)\n",
    "plt.ylabel('overall_score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn can perform a similar procedure for us using binning (or discretization).\n",
    "\n",
    "*Note*: the \"ideal\" plot for this from our visualization repertoire is 14 boxplots stacked horizontally - however, this ends up masking the trends due to a *lot* of outliers and too much variance to draw conclusions about the mean. Keep in mind that we've \"masked\" the variance of the data here a lot more (\"hidden\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data = df, x = 'disc_score', y = 'overall_score', x_bins = 14, fit_reg = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our `correlation` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(df['disc_score'], df['overall_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an in-built `correlation` function:\n",
    "* The matrix elements are symmetric: elements are correlations of (x, x), (x, y), (y, x), and (y, y).\n",
    "* NumPy and pandas functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y)*np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat = intercept(df['disc_score'], df['overall_score'])\n",
    "bhat = slope(df['disc_score'], df['overall_score'])\n",
    "\n",
    "print(\"predicted overall score = {} + {} * discussion score\".format(np.round(ahat, 2), np.round(bhat, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our linear model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(data = df, x = 'disc_score', y = 'overall_score', x_bins = 14, fit_reg = False)\n",
    "plt.plot(np.linspace(0, 14, 15), bhat * np.linspace(0, 14, 15) + ahat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def fit_least_squares(x, y):\n",
    "    ahat = intercept(x, y)\n",
    "    bhat = slope(x, y)\n",
    "    return ahat, bhat\n",
    "\n",
    "def predict(x, ahat, bhat):\n",
    "    return ahat + bhat*x\n",
    "\n",
    "def compute_mse(y, yhat):\n",
    "    return np.mean((y - yhat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define `least_squares_evaluation` which:\n",
    "* Computes general data statistics like mean, standard deviation, and linear correlation $r$\n",
    "* Fits least squares to data of the form $(x, y)$\n",
    "* Computes performance metrics like RMSE\n",
    "* Optionally plots two visualizations:\n",
    "    * Original scatter plot with fitted line\n",
    "    * Residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # revert style to default mpl\n",
    "NO_VIZ, RESID, RESID_SCATTER = range(3)\n",
    "def least_squares_evaluation(x, y, visualize=NO_VIZ):\n",
    "    # statistics\n",
    "    print(f\"x_mean : {np.mean(x):.2f}, y_mean : {np.mean(y):.2f}\")\n",
    "    print(f\"x_stdev: {np.std(x):.2f}, y_stdev: {np.std(y):.2f}\")\n",
    "    print(f\"r = Correlation(x, y): {correlation(x, y):.3f}\")\n",
    "    \n",
    "    # performance metrics\n",
    "    ahat, bhat = fit_least_squares(x, y)\n",
    "    yhat = predict(x, ahat, bhat)\n",
    "    print(f\"ahat: {ahat:.2f}, bhat: {bhat:.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(compute_mse(y, yhat)):.3f}\")\n",
    "\n",
    "    # visualization\n",
    "    fig, ax_resid = None, None\n",
    "    if visualize == RESID_SCATTER:\n",
    "        fig, axs = plt.subplots(1,2,figsize=(8, 3))\n",
    "        axs[0].scatter(x, y)\n",
    "        axs[0].plot(x, yhat)\n",
    "        axs[0].set_title(\"LS fit\")\n",
    "        ax_resid = axs[1]\n",
    "    elif visualize == RESID:\n",
    "        fig = plt.figure(figsize=(4, 3))\n",
    "        ax_resid = plt.gca()\n",
    "    \n",
    "    if ax_resid is not None:\n",
    "        ax_resid.scatter(x, y - yhat, color = 'red')\n",
    "        ax_resid.plot([4, 14], [0, 0], color = 'black')\n",
    "        ax_resid.set_title(\"Residuals\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try just doing linear fit *without* visualizing data.\n",
    "\n",
    "**Note**: Computation without visualization is NOT a good practice! We are doing the three evaluation steps **out of order** to highlight the importance of visualization.\n",
    "\n",
    "Here are the evaluation steps **in order**:\n",
    "1. Visualize original data, compute statistics\n",
    "2. If it seems reasonable, fit linear model\n",
    "3. Finally, compute performance metrics of linear model and plot residuals and other visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in four different datasets: I, II, III, IV\n",
    "anscombe = sns.load_dataset('anscombe')\n",
    "anscombe['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute statistics and performance metrics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[anscombe['dataset'] == dataset]\n",
    "    least_squares_evaluation(ans['x'], ans['y'], visualize=NO_VIZ)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, looks like all four datasets have the same:\n",
    "* statistics of $x$ and $y$\n",
    "* correlation $r$\n",
    "* regression line parameters $\\hat{a}, \\hat{b}$\n",
    "* RMSE (average squared loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[anscombe['dataset'] == dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize=RESID)\n",
    "    plt.show(fig)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the original data (what we should have done at the beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[anscombe['dataset'] == dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize=RESID_SCATTER)\n",
    "    plt.show(fig)\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
