{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction Using Derived Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the `tips` dataset. Each row represents one table that ate at a restaurant. For example, the top row of the table was a table for 2 eating dinner on a Sunday, where the person who paid the check was female and not a smoker. This table had a \\\\$16.99 total bill, and they tipped \\\\$1.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"tips\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cultures, it is customary to tip based on the total bill. This data appears to have been collected from such a culture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x = \"total_bill\", y = \"tip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to create a linear regression model that predicts the tip from the total bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = a + b x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{a}$ and $\\hat{b}$ be the choices that minimize the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to compute $\\hat{a}$ and $\\hat{b}$ is to use the equations that we derived in Lecture 9:\n",
    "\n",
    "$$\\hat{a} = \\bar{y} - \\hat{b} \\bar{x}$$\n",
    "\n",
    "$$\\hat{b} = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "$$r = \\frac{1}{n} \\sum_{i=1}^{n}\\left( \\frac{x_i - x}{\\sigma_x} \\right) \\left( \\frac{y_i - y}{\\sigma_y} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"tip\"]\n",
    "x = df[\"total_bill\"]\n",
    "y_bar = np.mean(y)\n",
    "x_bar = np.mean(x)\n",
    "sigma_y = np.std(y)\n",
    "sigma_x = np.std(x)\n",
    "r = np.sum((x - x_bar) / sigma_x * (y - y_bar) / sigma_y) / len(x)\n",
    "b_hat = r * sigma_y / sigma_x\n",
    "a_hat = y_bar - b_hat * x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hat = r * sigma_y / sigma_x\n",
    "b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = y_bar - b_hat * x_bar\n",
    "a_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predicted_tip\"] = a_hat + b_hat * df[\"total_bill\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"residual\"] = df[\"tip\"] - df[\"predicted_tip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df[\"residual\"]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn package (more on that soon) provides a function that computes the MSE from a list of observations and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df[\"tip\"], df[\"predicted_tip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the output of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x = \"total_bill\", y = \"tip\")\n",
    "plt.plot(df[\"total_bill\"], df[\"predicted_tip\"], 'r')\n",
    "plt.legend([r'$\\hat{y}$', '$y$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "scatter_plot = px.scatter(df, x=\"total_bill\", y=\"tip\")\n",
    "line_plot = px.line(df, x=\"total_bill\", y=\"predicted_tip\")\n",
    "line_plot.update_traces(line=dict(color = 'rgba(255, 0, 0)'))\n",
    "\n",
    "go.Figure(data=scatter_plot.data + line_plot.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression Model Training and Prediction Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate approach to optimize our model is to use the `sklearn.linear_model.LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a model. At this point it is like a baby, knowing nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we \"fit\" the model, which means computing the parameters that minimize the loss function. The `LinearRegression` class is hard coded to use the MSE as its loss function. The first argument of the fit function should be a matrix (or DataFrame), and the second should be the observation we're trying to predict, which in this case is a series of tip values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df[[\"total_bill\"]], df[\"tip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can ask it questions. The code below asks the model to estimate the tip for a table with a 20 dollar total bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the model to generate a series of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sklearn_predictions\"] = model.predict(df[[\"total_bill\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the predictions, we see that they are exactly the same as we got using our analytic formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the model for its intercept and slope with `_intercept` and `_coef`. They are the same as with our analytic formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prediction Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by removing the columns we added earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"predicted_tip\", \"residual\", \"sklearn_predictions\"], axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to compute the tip using both the total_bill and the size of the party. We can do this easily in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2d = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2d.fit(df[[\"total_bill\", \"size\"]], df[\"tip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this model, we can make predictions. For example, we can ask our model about the tip for a table with a total bill of 10 dollars and 3 customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2d.predict([[10, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this prediction is different than our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Using the Derived Equation for a 2D Linear Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation used by this model is the multiple linear regression model described in this slide: https://docs.google.com/presentation/d/1HZu4wK3wcG81ldQ-7edyunvwVXq4LBJCdXviyi3ZW9c/edit#slide=id.g113dfce000f_0_2682\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
    "\n",
    "\n",
    "We can get $\\theta_0$ with `intercept_` and $\\theta_1$ and $\\theta_2$ with `coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"theta0: {model_2d.intercept_}\")\n",
    "print(f\"theta1 and theta2: {model_2d.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the model prediction as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6689 + 0.0927 * 10 + 0.1926 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Using the Matrix Form for a Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually compute the predictions using matrix multiplication, using the equation from https://docs.google.com/presentation/d/15eEbroVt2r36TXh28C2wm6wgUHlCBCsODR09kLHhDJ8/edit#slide=id.g113dfce000f_0_1309:\n",
    "\n",
    "$$ \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[0.6689, 0.0927, 0.1926]]).T\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 10, 3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the matrix form we can make many peredictions at once, for example, let X be the first four observations from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[[\"total_bill\", \"size\"]].head(4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to use matrix multiplication, we run into an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line\n",
    "#X.values @ theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's missing is our bias term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"total_bill\", \"size\"]].head(4)\n",
    "X[\"bias\"] = [1, 1, 1, 1]\n",
    "X = X[[\"bias\", \"total_bill\", \"size\"]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same values that we get if we use `.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2d.predict(X[[\"total_bill\", \"size\"]].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Our 2D Linear Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df[\"total_bill\"], df[\"size\"], df[\"tip\"])\n",
    "plt.xlabel('total bill')\n",
    "plt.ylabel('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df[\"total_bill\"], df[\"size\"], df[\"tip\"])\n",
    "xx, yy = np.meshgrid(range(50), range(6))\n",
    "zz = ( 0.6689 + 0.0927 * xx + 0.1926 * yy)\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.2)\n",
    "plt.xlabel('total bill')\n",
    "plt.ylabel('size')\n",
    "plt.gcf().savefig(\"hi.png\", dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions all lie in a plane. In higher dimensions, they all lie in a \"hyperplane\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model Training Using the Derived Matrix Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our one dimensional model, we computed $\\hat{a}$ and $\\hat{b}$ using the equations for simple linear regression.\n",
    "\n",
    "For a multi-dimensional model we can use the normal equation, which we derived in Lecture 11: https://docs.google.com/presentation/d/1HZu4wK3wcG81ldQ-7edyunvwVXq4LBJCdXviyi3ZW9c/edit#slide=id.g113dfce000f_0_2682\n",
    "\n",
    "$$\\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "Note that this derivation is one of the most challenging in the course, especially for those of you who are learning linear algebra during the same semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"total_bill\", \"size\"]].copy()\n",
    "X[\"bias\"] = np.ones(len(X))\n",
    "X = X[[\"bias\", \"total_bill\", \"size\"]]\n",
    "X.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[[\"tip\"]]\n",
    "Y.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameters we computed using sklearn were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we use the normal equation, we get back:\n",
    "\n",
    "$$\\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_using_normal_equation = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "theta_using_normal_equation.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code above is inefficient. We won't go into this in our class, but it's better to use `np.linalg.solve` rather than computing the explicit matrix inverse.\n",
    "\n",
    "Note also: sklearn does NOT use the normal equation, instead it uses a procedure called gradient descent which can minimize ANY function, not just the MSE. The rest of the lecture will explain how gradient descent works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing an Arbitrary 1D Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to find the minimum of the arbitrary function given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arbitrary(x):\n",
    "    return (x**4 - 15*x**3 + 80*x**2 - 180*x + 144)/10\n",
    "\n",
    "x = np.linspace(1, 6.75, 200)\n",
    "fig = px.line(y = arbitrary(x), x = x)\n",
    "\n",
    "\n",
    "fig.update_layout(font_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to minimize this mathematical function is to use the `scipy.optimize.minimize` function. It takes a function and a starting guess and tries to find the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(arbitrary, x0 = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of start point can affect the outcome. For example if we start to the left, we get stuck in the local minimum on the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(arbitrary, x0 = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.minimize` is great. It may also seem a bit magical. How could you write a function that can find the minimum of any mathematical function? There are a number of ways to do this, which we'll explore in today's lecture, eventually arriving at the important idea of **gradient descent**, which is the principle that `scipy.optimize.minimize` uses.\n",
    "\n",
    "It turns out that under the hood, the `fit` method for `LinearRegression` models uses gradient descent. Gradient descent is also how much of machine learning works, including even advanced neural network models. \n",
    "\n",
    "In DS100, the gradient descent process will usually be invisible to us, hidden beneath an abstraction layer. However, to be good data scientists, it's important that we know the basic principles beyond the optimization functions that harness to find optimal parmaeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Approach: Guess and Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we saw that the minimum is somewhere around 5.3ish. Let's see if we can figure out how to find the exact minimum algorithmically from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way very slow and terrible way would be manual guess-and-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A somewhat better approach is to use brute force to try out a bunch of x values and return the one that yields the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_minimize(f, xs):\n",
    "    y = [f(x) for x in xs]  \n",
    "    return xs[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_minimize(arbitrary, np.linspace(1, 7, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is essentially the same as before where we made a graphical plot, it's just that we're only looking at 20 selected points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 7, 200)\n",
    "sparse_xs = np.linspace(1, 7, 5)\n",
    "\n",
    "ys = arbitrary(xs)\n",
    "sparse_ys = arbitrary(sparse_xs)\n",
    "\n",
    "fig = px.line(x = xs, y = arbitrary(xs))\n",
    "fig.add_scatter(x = sparse_xs, y = arbitrary(sparse_xs), mode = \"markers\")\n",
    "fig.update_layout(showlegend= False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic approach suffers from three major flaws:\n",
    "1. If the minimum is outside our range of guesses, the answer will be completely wrong.\n",
    "2. Even if our range of guesses is correct, if the guesses are too coarse, our answer will be inaccurate.\n",
    "3. It is absurdly computationally inefficient, considering potentially vast numbers of guesses that are useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Approach: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of choosing all of our guesses ahead of time, we can instead start from a single guess and try to iteratively improve on our choice. \n",
    "\n",
    "They key insight is this: If the derivative of the function is negative, that means the function is decreasing, so we should go to the right (i.e. pick a bigger x). If the derivative of the function is positive, that means the function is increasing, so we should go to the left (i.e. pick a smaller x).\n",
    "\n",
    "Thus, the derivative tells us which way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desmos demo: https://www.desmos.com/calculator/twpnylu4lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def derivative_arbitrary(x):\n",
    "    return (4*x**3 - 45*x**2 + 160*x - 180)/10\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "roots = np.array([2.3927, 3.5309, 5.3263])\n",
    "\n",
    "fig.add_trace(go.Scatter(x = xs, y = arbitrary(xs), \n",
    "                         mode = \"lines\", name = \"f\"))\n",
    "fig.add_trace(go.Scatter(x = xs, y = derivative_arbitrary(xs), \n",
    "                         mode = \"lines\", name = \"df\", line = {\"dash\": \"dash\"}))\n",
    "fig.add_trace(go.Scatter(x = np.array(roots), y = 0*roots, \n",
    "                         mode = \"markers\", name = \"df = zero\", marker_size = 12))\n",
    "fig.update_layout(font_size = 20, yaxis_range=[-1, 3])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_arbitrary():\n",
    "    x = np.linspace(1, 7, 100)\n",
    "    plt.plot(x, arbitrary(x))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-1, 3])\n",
    "    \n",
    "def plot_x_on_f(f, x):\n",
    "    y = f(x)    \n",
    "    default_args = dict(label=r'$ \\theta $', zorder=2,\n",
    "                        s=200, c=sns.xkcd_rgb['green'])\n",
    "    plt.scatter([x], [y], **default_args)\n",
    "    \n",
    "def plot_x_on_f_empty(f, x):\n",
    "    y = f(x)\n",
    "    default_args = dict(label=r'$ \\theta $', zorder=2,\n",
    "                        s=200, c = 'none', edgecolor=sns.xkcd_rgb['green'])\n",
    "    plt.scatter([x], [y], **default_args)    \n",
    "    \n",
    "def plot_tangent_on_f(f, x, eps=1e-6):\n",
    "    slope = ((f(x + eps) - f(x - eps))\n",
    "             / (2 * eps))\n",
    "    xs = np.arange(x - 1, x + 1, 0.05)\n",
    "    ys = f(x) + slope * (xs - x)\n",
    "    plt.plot(xs, ys, zorder=3, c=sns.xkcd_rgb['green'], linestyle='--')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arbitrary()\n",
    "plot_x_on_f(arbitrary, 2)\n",
    "plot_tangent_on_f(arbitrary, 2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arbitrary()\n",
    "plot_x_on_f(arbitrary, 4.4)\n",
    "plot_tangent_on_f(arbitrary, 4.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Descending the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this knowledge, let's try to see if we can use the derivative to optimize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = 4\n",
    "print(f\"x: {guess}, f(x): {arbitrary(guess)}, derivative f'(x): {derivative_arbitrary(guess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = 4 + 0.4\n",
    "print(f\"x: {guess}, f(x): {arbitrary(guess)}, derivative f'(x): {derivative_arbitrary(guess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_step(x):\n",
    "    new_x = x - derivative_arbitrary(x)\n",
    "    plot_arbitrary()\n",
    "    plot_x_on_f(arbitrary, new_x)\n",
    "    plot_x_on_f_empty(arbitrary, x)\n",
    "    print(f'old x: {x}')\n",
    "    print(f'new x: {new_x}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(4.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(5.0464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(5.4967)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(5.080917145374805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step(5.489966698640582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_step_better(x):\n",
    "    new_x = x - 0.3 * derivative_arbitrary(x)\n",
    "    plot_arbitrary()\n",
    "    plot_x_on_f(arbitrary, new_x)\n",
    "    plot_x_on_f_empty(arbitrary, x)\n",
    "    print(f'old x: {x}')\n",
    "    print(f'new x: {new_x}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step_better(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step_better(4.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step_better(5.17180969114245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_step_better(5.323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent as an Algorithmic Procedure in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written as a recurrence relation, the process we've described above is:\n",
    "\n",
    "$$\n",
    "x^{(t+1)} = x^{(t)} -  0.3 \\frac{d}{dx} f(x)\n",
    "$$\n",
    "\n",
    "This algorithm is also known as \"gradient descent\". \n",
    "\n",
    "Given a current $x$, gradient descent creates its next guess for $x$ based on the sign and magnitude of the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of 0.3 above was totally arbitrary. Naturally, we can generalize by replacing it with a parameter, typically represented by $\\alpha$, and often called the \"learning rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{(t+1)} = x^{(t)} -  \\alpha \\frac{d}{dx} f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write up this procedure in code as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, initial_guess, alpha, n):\n",
    "    \"\"\"Performs n steps of gradient descent on df using learning rate alpha starting\n",
    "       from initial_guess. Returns a numpy array of all guesses over time.\"\"\"\n",
    "    guesses = [initial_guess]\n",
    "    current_guess = initial_guess\n",
    "    while len(guesses) < n:\n",
    "        current_guess = current_guess - alpha * df(current_guess)\n",
    "        guesses.append(current_guess)\n",
    "        \n",
    "    return np.array(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectory = gradient_descent(derivative_arbitrary, 4, 0.3, 20)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = gradient_descent(derivative_arbitrary, 4, 1, 20)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see a visualization of the trajectory taken by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = gradient_descent(derivative_arbitrary, 1.6, 0.6, 20)\n",
    "plot_arbitrary()\n",
    "plt.plot(trajectory, arbitrary(trajectory));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've simply run our algorithm a fixed number of times. More sophisticated implementations will stop based on a variety of different stopping criteria, e.g. error getting too small, error getting too large, etc. We will not discuss these in our course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll return to the world of real data and see how this procedure might be useful for optimizing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression With No Offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a case where we have a linear model with no offset. \n",
    "\n",
    "$$\\hat{y} = \\theta_1 x$$\n",
    "\n",
    "We want to find the parameter $\\theta_1$ such that the L2 loss is minimized. In sklearn, this is easy. To avoid fitting an intercept, we set `fit_intercept` to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept = False)\n",
    "model.fit(df[[\"total_bill\"]], df[\"tip\"])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal tip percentage is 14.37%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an Explicit MSE Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To employ gradient descent and do this ourselves, we need to define a function upon which we can use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we select the L2 loss as our loss function. In this case, our goal will be to minimize the mean squared error. \n",
    "\n",
    "Let's start by writing a function that computes the MSE for a given choice of $\\theta_1$ on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta1, x, y_obs):\n",
    "    y_hat = theta1 * x\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"total_bill\"]\n",
    "y_obs = df[\"tip\"]\n",
    "mse_loss(0.147, x, y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `x` and `y_obs` are fixed, the only variable is `theta1`. \n",
    "\n",
    "For clarity, let's define a python function that returns the MSE as a function of a single argument `theta1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_single_arg(theta1):\n",
    "    \"\"\"Returns the MSE on our data for the given theta1\"\"\"\n",
    "    x = df[\"total_bill\"]\n",
    "    y_obs = df[\"tip\"]\n",
    "    y_hat = theta1 * x\n",
    "    return np.mean((y_hat - y_obs) ** 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_single_arg(0.1437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brute Forcing our Explicit MSE Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can plot the MSE as a function of `theta1`. It turns out to look pretty smooth, and quite similar to a parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1s = np.linspace(0, 0.2, 200)\n",
    "x = df[\"total_bill\"]\n",
    "y_obs = df[\"tip\"]\n",
    "\n",
    "MSEs = [mse_single_arg(theta1) for theta1 in theta1s]\n",
    "\n",
    "plt.plot(theta1s, MSEs)\n",
    "plt.xlabel(r\"Choice for $\\theta_1$\")\n",
    "plt.ylabel(r\"MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum appears to be around $\\theta_1 = 0.14$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our simple_minimize function from earlier, redefined below for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_minimize(f, xs):\n",
    "    y = [f(x) for x in xs]  \n",
    "    return xs[np.argmin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_minimize(mse_single_arg, np.linspace(0, 0.2, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, what we're doing is computing all the starred values below and then returning the $\\theta_1$ that goes with the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1s = np.linspace(0, 0.2, 200)\n",
    "sparse_theta1s = np.linspace(0, 0.2, 21)\n",
    "\n",
    "loss = [mse_single_arg(theta1) for theta1 in theta1s]\n",
    "sparse_loss = [mse_single_arg(theta1) for theta1 in sparse_theta1s]\n",
    "\n",
    "plt.plot(theta1s, loss)\n",
    "plt.plot(sparse_theta1s, sparse_loss, 'r*')\n",
    "plt.xlabel(r\"Choice for $\\theta_1$\")\n",
    "plt.ylabel(r\"MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Scipy.Optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "from scipy.optimize import minimize\n",
    "minimize(mse_single_arg, x0 = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Our Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use our 1D gradient descent algorithm from earlier. This is the exact same function as earlier, but copy and pasted to remind you what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, initial_guess, alpha, n):\n",
    "    \"\"\"Performs n steps of gradient descent on df using learning rate alpha starting\n",
    "       from initial_guess. Returns a numpy array of all guesses over time.\"\"\"\n",
    "    guesses = [initial_guess]\n",
    "    current_guess = initial_guess\n",
    "    while len(guesses) < n:\n",
    "        current_guess = current_guess - alpha * df(current_guess)\n",
    "        guesses.append(current_guess)\n",
    "        \n",
    "    return np.array(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, we need to compute the derivative of the MSE. The MSE is repeated below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_single_arg(theta1):\n",
    "    \"\"\"Returns the MSE on our data for the given theta1\"\"\"\n",
    "    x = df[\"total_bill\"]\n",
    "    y_obs = df[\"tip\"]\n",
    "    y_hat = theta1 * x\n",
    "    return np.mean((y_hat - y_obs) ** 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the derivative is therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_derivative_single_arg(theta_1):\n",
    "    \"\"\"Returns the derivative of the MSE on our data for the given theta1\"\"\"\n",
    "    x = df[\"total_bill\"]\n",
    "    y_obs = df[\"tip\"]\n",
    "    y_hat = theta_1 * x\n",
    "    \n",
    "    return np.mean(2 * (y_hat - y_obs) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(mse_loss_derivative_single_arg, 0.05, 0.0001, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of minimizing loss, we can write out the gradient descent rule for generating the next $\\theta_1$ as:\n",
    "\n",
    "$$\n",
    "\\theta_1^{(t+1)} = \\theta_1^{(t)} - \\alpha \\frac{\\partial}{\\partial \\theta_1} L(\\theta^{(t)}, \\Bbb{X}, \\vec{\\hat{y}})\n",
    "$$\n",
    "\n",
    "Here $L$ is our chosen loss function, $\\Bbb{X}$ are the features of our input, and $\\vec{\\hat{y}}$ are our observations. During the gradient descent algorithm, we treat $\\Bbb{X}$ and $\\vec{\\hat{y}}$ as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Dimensional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we improve our model so that we want to predict the tip from the total_bill plus a constant offset, in other words:\n",
    "\n",
    "$$\\textrm{tip} = \\theta_0 + \\theta_1 \\textrm{bill}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to do this in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept = True)\n",
    "X = df[[\"total_bill\"]]\n",
    "y = df[\"tip\"]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sklearn (explicit bias column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our lives easier, let's reframe the problem slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_with_bias = df.copy()\n",
    "tips_with_bias[\"bias\"] = 1\n",
    "X = tips_with_bias[[\"bias\", \"total_bill\"]]\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept = False)\n",
    "X = tips_with_bias[[\"bias\", \"total_bill\"]]\n",
    "y = df[\"tip\"]\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a 2D MSE Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can give our predictions as $$\\hat{y} = \\theta_0 + \\theta_1 \\times bill$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the predictions below correspond to assuming every table leaves a tip of \\$1.50 plus 5% of their total bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X @ np.array([1.5, 0.05]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this problem, we'll assume we want to minimize the mean squared error of our predictions, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta, X, y_obs):\n",
    "    y_hat = X @ theta\n",
    "    return np.mean((y_hat - y_obs) ** 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the loss assuming the model described above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss(np.array([1.5, 0.05]), X, y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can create a 3D plot. This uses lots of syntax you've never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "uvalues = np.linspace(0, 2, 10)\n",
    "vvalues = np.linspace(0, 0.2, 10)\n",
    "(u,v) = np.meshgrid(uvalues, vvalues)\n",
    "thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "def mse_loss_single_arg(theta):\n",
    "    return mse_loss(theta, X, y_obs)\n",
    "\n",
    "MSE = np.array([mse_loss_single_arg(t) for t in thetas.T])\n",
    "\n",
    "loss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n",
    "\n",
    "ind = np.argmin(MSE)\n",
    "optimal_point = go.Scatter3d(name = \"Optimal Point\",\n",
    "    x = [thetas.T[ind,0]], y = [thetas.T[ind,1]], \n",
    "    z = [MSE[ind]],\n",
    "    marker=dict(size=10, color=\"red\"))\n",
    "\n",
    "fig = go.Figure(data=[loss_surface, optimal_point])\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title = \"theta0\",\n",
    "    yaxis_title = \"theta1\",\n",
    "    zaxis_title = \"MSE\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the 2D Model With Our Own Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function we just derived, but in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(theta, X, y_obs):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"    \n",
    "    x0 = X.iloc[:, 0]\n",
    "    x1 = X.iloc[:, 1]\n",
    "    dth0 = np.mean(-2 * (y_obs - theta[0]*x0 - theta[1]*x1) * x0)\n",
    "    dth1 = np.mean(-2 * (y_obs - theta[0]*x0 - theta[1]*x1) * x1)\n",
    "    return np.array([dth0, dth1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_with_bias[[\"bias\", \"total_bill\"]]\n",
    "y_obs = tips_with_bias[\"tip\"]\n",
    "mse_gradient(np.array([0, 0]), X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient_single_arg(theta):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"\n",
    "    X = tips_with_bias[[\"bias\", \"total_bill\"]]\n",
    "    y_obs = tips_with_bias[\"tip\"]\n",
    "    return mse_gradient(theta, X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_gradient_single_arg(np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "guesses = gradient_descent(mse_gradient_single_arg, np.array([0, 0]), 0.001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(guesses).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around with the code above, you'll see that it's pretty finicky about the start point and learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the general matrix form of the gradient is given below. We have not discussed how to derive this in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(theta, X, y_obs):\n",
    "    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"\n",
    "    n = len(X)\n",
    "    return -2 / n * (X.T @ y_obs - X.T @ X @ theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
