{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Synthetic Data\n",
    "\n",
    "For this lecture we use a simple synthetic dataset to simplify the presentation of ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "np.random.seed(42)\n",
    "noise = 0.7\n",
    "w_true = np.array([1., 3.])\n",
    "quad = -4\n",
    "\n",
    "x = np.sort(np.random.rand(n)*2 - 1.)\n",
    "y = w_true[0] + w_true[1] * x + quad*(x**2) + noise * np.random.randn(n)\n",
    "x[0] = -1.5\n",
    "y[0] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_plot = go.Scatter(x=x, y=y, name=\"Raw Data\", mode='markers')\n",
    "fig = go.Figure([raw_data_plot])\n",
    "fig.update_layout(height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining The Model and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, x):\n",
    "    return w[0] + w[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous lecture we showed how to analytically derive the minimizer for the squared loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "def correlation(x, y):\n",
    "    return (standard_units(x) * standard_units(y)).mean()\n",
    "\n",
    "def slope(x, y):\n",
    "    return correlation(x, y) * y.std() / x.std()\n",
    "\n",
    "def intercept(x, y):\n",
    "    return y.mean() - slope(x, y)*x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the weights based on the functions from prior lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse = np.array([intercept(x,y), slope(x,y)])\n",
    "w_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(w_mse, x)\n",
    "analytic_mse_line = go.Scatter(x=x,y=y_hat, name=\"Analytic MSE\")\n",
    "fig = go.Figure([raw_data_plot, analytic_mse_line])\n",
    "fig.update_layout(height=700)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for Plotly only to generate residual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_lines(x, y, yhat):\n",
    "    return [ \n",
    "        go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "        for (x, y, yhat) in zip(x, y, y_hat)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y =   y -y_hat , mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Loss Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(yhat, y):\n",
    "    return ((yhat - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustively try a large number of possible parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Range of intercpet values\n",
    "w0values = np.linspace(w_mse[0]-1, w_mse[0]+1, 50) \n",
    "# Compute Range of Slope values\n",
    "w1values = np.linspace(w_mse[1]-1, w_mse[1]+1, 50)\n",
    "# Construct \"outer product of all possible values\"\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "# Convert into a tall matrix with each row corresponding to a possible parameterization\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "# Compute the Loss for each parameterization\n",
    "mse_loss_values = np.array([mse_loss(y, model(w, x)) for w in ws]).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a really cool looking visualization of the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    specs=[[{'type': 'contour'}, {'type': 'surface'}]])\n",
    "# Make Contour Plot and Point\n",
    "fig.add_trace(go.Contour(x=w0values, y=w1values, z=mse_loss_values , colorbar=dict(x=-.1)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[w_mse[0]], y=[w_mse[1]]), row=1, col=1)\n",
    "# Make Surface Plot and Point\n",
    "fig.add_trace(go.Surface(x=w0values, y=w1values, z=mse_loss_values, opacity=0.9), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=[w_mse[0]], y=[w_mse[1]], z=[mse_loss(y, model(w_mse,x))]), row=1, col=2)\n",
    "# Cleanup Legend\n",
    "fig.update_layout(scene=dict(xaxis=dict(title='Slope'), yaxis=dict(title='Intercept'), zaxis=dict(title=\"MSE Loss\")))\n",
    "fig.update_xaxes(title_text=\"Intercept\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Slope\", row=1, col=1)\n",
    "fig.update_layout(height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the $L^1$ Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just solved the model for the $L^2$ loss.  We now examine the $L^1$ loss.  We first begin by visualizing the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(yhat, y):\n",
    "    return (np.abs(yhat - y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Range of intercpet values\n",
    "w0values = np.linspace(w_mse[0]-1, w_mse[0]+1, 50) \n",
    "# Compute Range of Slope values\n",
    "w1values = np.linspace(w_mse[1]-1, w_mse[1]+1, 50)\n",
    "# Construct \"outer product of all possible values\"\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "# Convert into a tall matrix with each row corresponding to a possible parameterization\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "# Compute the Loss for each parameterization\n",
    "abs_loss_values = np.array([abs_loss(y, model(w, x)) for w in ws]).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    specs=[[{'type': 'contour'}, {'type': 'surface'}]])\n",
    "# Make Contour Plot and Point\n",
    "fig.add_trace(go.Contour(x=w0values, y=w1values, z=abs_loss_values, colorbar=dict(x=-.1)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[w_mse[0]], y=[w_mse[1]]), row=1, col=1)\n",
    "# Make Surface Plot and Point\n",
    "fig.add_trace(go.Surface(x=w0values, y=w1values, z=abs_loss_values, opacity=0.9), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=[w_mse[0]], y=[w_mse[1]], z=[abs_loss(y, model(w_mse,x))]), row=1, col=2)\n",
    "# Cleanup Legend\n",
    "fig.update_layout(scene=dict(xaxis=dict(title='Slope'), yaxis=dict(title='Intercept'), zaxis=dict(title=\"MSE Loss\")))\n",
    "fig.update_xaxes(title_text=\"Intercept\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Slope\", row=1, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([raw_data_plot, analytic_mse_line])\n",
    "fig.update_layout(height=700)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Introduction to Algorithmic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we are going to introduce PyTorch.  PyTorch is sort of like learning how to use Thor's hammer, it is way overkill for basically everything you will do and is probably the wrong solution to most problems you will encounter. However, it also really powerful and will give you the skills needed to take on very challenging problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a variable $\\theta$ with an initial value 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor([1.0], requires_grad=True, dtype=torch.float64)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we compute the following value from our tensor `theta`\n",
    "\n",
    "$$\n",
    "z = \\left(1 - log\\left(1 + \\exp(\\theta) \\right) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (1 - torch.log(1 + torch.exp(theta)))**2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every derived value has an attached a gradient function that is used to compute the backwards path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz\n",
    "# !brew install graphviz\n",
    "# from torchviz import make_dot\n",
    "# make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were unable to run the above cell here is what the output looks like:\n",
    "\n",
    "<img src=\"torch_graph.png\" alt=\"TorchGraph\" style=\"height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These backward functions tell Torch how to compute the gradient via the chain rule.  This is done by invoking backward on the computed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `item` to extract a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this witht he hand computed derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial\\theta} &= \\frac{\\partial}{\\partial\\theta}\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)^2 \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\frac{\\partial}{\\partial\\theta} \\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right) (-1) \\frac{\\partial}{\\partial\\theta} \\log\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\frac{\\partial}{\\partial\\theta}\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\exp(\\theta) \\\\\n",
    "  & = -2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_derivative(theta):\n",
    "    return -2 * (1 - np.log(1 + np.exp(theta))) * np.exp(theta) / (1. + np.exp(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_derivative(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the Absolute Loss Using Gradient Descent\n",
    "\n",
    "Here we will use pytorch to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy data to tensors.  I will call them `tx` and `ty` to reduce confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Model \n",
    "\n",
    "The following defines a simple linear model as a basic class.  The `init` function initializes the weights of the model.  Here we have two weights, intercept and slope.  The weights are initialized with `requires_grad` set to true so PyTorch will track the gradients of these weights.\n",
    "\n",
    "The `predict` function makes a prediction based on the input `x` and the weights.  In general, `x` will contain one or more inputs and so the function should work in either case. \n",
    "\n",
    "the `__call__` function allows me to call `model.predict(x)` as `model(x)` treating my models as a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel:\n",
    "    def __init__(self):\n",
    "        self.w = torch.zeros(2, 1, requires_grad=True)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        w = self.w\n",
    "        return w[0] + w[1] * x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinearModel()\n",
    "model(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PyTorch `nn.Module`\n",
    "The ideal way to define a model in pytorch is to extend the `nn.Module` class and introduce `Parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, w=None):\n",
    "        super().__init__()\n",
    "        # Creating a nn.Parameter object allows torch to track parameters for us\n",
    "        if w is not None: \n",
    "            self.w = nn.Parameter(torch.from_numpy(w))\n",
    "        else: \n",
    "            self.w = nn.Parameter(torch.zeros(2,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w = self.w\n",
    "        return w[0] + w[1] * x       \n",
    "    \n",
    "    # Prof Gonzalez\n",
    "    def numpy_parameters(self):\n",
    "        \"\"\"Return a numpy version of the parameters.\"\"\"\n",
    "        return np.array([p.detach().numpy() for p in self.parameters()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = SimpleLinearModel()\n",
    "lin_model(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Module` class has some nice helper functions.  For example, any parameters members of a module are automatically captured.  This will be useful when we design modules with many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in lin_model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added the `numpy_parameters` method to construct a single parameter numpy vector for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.numpy_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Loss\n",
    "\n",
    "There are many built in loss functions but we will build our own to see how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss_torch(ypred, y):\n",
    "    return torch.abs(ypred - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = abs_loss_torch(lin_model(tx), ty)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item method returns the actual value from a single value tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.w.grad\n",
    "# [p.grad for p in lin_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_model.w.grad.zero_() # <- this also works\n",
    "lin_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.grad for p in lin_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a library of many loss functions in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.l1_loss(lin_model(tx), ty)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements a basic version of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model, loss_fn, x, y, lr=1., nsteps=100):\n",
    "    values = [model.numpy_parameters()] # Track parameter values for later viz.\n",
    "    for i in range(nsteps):\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p -= lr / (i + 1) * p.grad\n",
    "            model.zero_grad()\n",
    "            values.append(model.numpy_parameters())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = SimpleLinearModel()\n",
    "values = gradient_descent(lin_model, F.l1_loss, tx, ty, nsteps=500, lr=6.0)\n",
    "print(\"Loss =\", F.l1_loss(ty, lin_model(tx)).item())\n",
    "w_abs = lin_model.numpy_parameters()\n",
    "print(\"[intecept, slope] =\", w_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0values = np.linspace(-1., 1, 50)\n",
    "w1values = np.linspace(-.5, 5, 50)\n",
    "(u,v) = np.meshgrid(w0values, w1values)\n",
    "ws = np.vstack((u.flatten(),v.flatten())).transpose()\n",
    "loss = np.array([\n",
    "    F.l1_loss(ty, SimpleLinearModel(w)(tx)).item() for w in ws]).reshape(u.shape)\n",
    "\n",
    "fig = go.Figure([go.Contour(x=w0values, y=w1values, z=loss, colorbar=dict(x=-.2)),\n",
    "          go.Scatter(x=[w_mse[0]], y=[w_mse[1]], name=\"MSE\", mode=\"markers\"),\n",
    "          go.Scatter(x=[w_abs[0]], y=[w_abs[1]], name=\"Abs\", mode=\"markers\"),\n",
    "          go.Scatter(x=values[:,0], y=values[:,1], name=\"Path\", mode=\"markers+lines\", \n",
    "                     line=go.scatter.Line(color='white'))])\n",
    "fig.update_layout(height=600, xaxis_title=\"Intercept\", yaxis_title=\"Slope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the current best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lin_model(tx).detach().numpy()\n",
    "gd_abs_line = go.Scatter(x=x,y=y_hat, name=\"GD ABS\")\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line, gd_abs_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y = y - y_hat, mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving The Model\n",
    "\n",
    "In the above we notice some curvature in the residual plot and decide to build a more complex model with an extra quadratic term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialModel(nn.Module):\n",
    "    def __init__(self, w=None, p=2):\n",
    "        super().__init__()\n",
    "        # Creating a nn.Parameter object allows torch to track parameters for us\n",
    "        if w is not None: \n",
    "            self.w = nn.Parameter(torch.from_numpy(w))\n",
    "        else: \n",
    "            self.w = nn.Parameter(torch.zeros(p+1,1))\n",
    "    def forward(self, x):\n",
    "        w = self.w\n",
    "        return torch.sum(torch.stack([w[i] * x ** i for i in range(len(w))]), dim=0)\n",
    "    def numpy_parameters(self):\n",
    "        \"\"\"Return a numpy version of the parameters.\"\"\"\n",
    "        return np.array([p.detach().numpy() for p in self.parameters()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model = PolynomialModel(np.array([1., 2., 3.]))\n",
    "quad_model(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model = PolynomialModel(p=2)\n",
    "values = gradient_descent(quad_model, F.l1_loss, tx, ty, nsteps=500, lr=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, quad_model(tx)).item())\n",
    "w_quad_abs = quad_model.numpy_parameters()\n",
    "print(\"[intecept, slope, quad_term] =\", w_quad_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = quad_model(tx).detach().numpy().flatten()\n",
    "\n",
    "gd_quad_abs_line = go.Scatter(x=x, y=y_hat, name=\"GD Quad ABS\")\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "for t in residual_lines(x,y,y_hat) + [raw_data_plot, analytic_mse_line, gd_abs_line, gd_quad_abs_line]:\n",
    "    fig.add_trace(t, row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y = y - y_hat , mode='markers', name='Residuals'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=[x.min(), x.max()], y = [0,0], showlegend=False), row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Y\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a mechanism to sample the data.  Since this is central to SGD (and therefore PyTorch) there is a built in way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(tx, ty)\n",
    "loader = DataLoader(dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dict(x=x, y=y) for x, y in loader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic SGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(model, loss_fn, dataset, lr=1., nepochs=100, batch_size=10):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    values = [model.numpy_parameters()] # Track parameter values for later viz.\n",
    "    for i in range(nepochs):\n",
    "        for (x, y) in loader:\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            # We compute the update in a torch.no_grad context to prevent torch from \n",
    "            # trying to compute the gradient of the gradient calculation.\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= lr / (i + 1) * p.grad\n",
    "                # We also need to clear the gradient buffer otherwise future calls will\n",
    "                # accumulate the gradient. \n",
    "                model.zero_grad()\n",
    "                # print(i, loss.item())\n",
    "                values.append(model.numpy_parameters())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model_sgd = PolynomialModel(p=2)\n",
    "values = stochastic_gradient_descent(quad_model_sgd, F.l1_loss, dataset, \n",
    "                                     lr=1.0, nepochs=20, batch_size=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, quad_model_sgd(tx)).item())\n",
    "w_quad_abs_sgd = quad_model_sgd.numpy_parameters()\n",
    "print(\"[intecept, slope, quad_term] =\", w_quad_abs_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter3d(x=values[:,0], y=values[:,1], z=values[:,2], \n",
    "                              marker=dict(color=np.linspace(0,1,values.shape[0]))\n",
    "                             )])\n",
    "fig.update_layout(height = 800)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving SGD with better optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_sgd(model, loss_fn, dataset, lr=.1, nepochs=100, batch_size=10):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    values = [model.numpy_parameters()] # Track parameter values for later viz.\n",
    "    for i in range(nepochs):\n",
    "        for (x, y) in loader:\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            model.zero_grad()\n",
    "            values.append(model.numpy_parameters())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model_sgd2 = PolynomialModel(p=2)\n",
    "values = adam_sgd(quad_model_sgd2, F.l1_loss, dataset, lr = 0.1, nepochs=100, batch_size=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, quad_model_sgd2(tx)).item())\n",
    "w_quad_abs_sgd2 = quad_model_sgd2.numpy_parameters()\n",
    "print(\"[intecept, slope, quad_term] =\", w_quad_abs_sgd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_sgd = PolynomialModel(p=12)\n",
    "values = adam_sgd(poly_model_sgd, F.l1_loss, dataset, lr=0.01, nepochs=500, batch_size=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, poly_model_sgd(tx)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining this Amazing Fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(x.min(), x.max(), 1000)\n",
    "y_hat = poly_model_sgd(torch.from_numpy(xtest)).detach().numpy().flatten()\n",
    "\n",
    "poly_model_line = go.Scatter(x=xtest, y=y_hat, name=\"Poly Model Line\")\n",
    "fig = go.Figure([raw_data_plot, analytic_mse_line, gd_abs_line, gd_quad_abs_line, poly_model_line])\n",
    "fig.update_yaxes(title_text=\"Y\")\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Linear Model\n",
    "\n",
    "Here we make a model of the form:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\text{sigmoid}(\\theta_0 * (x - \\theta_1)) (\\theta_2 x + \\theta_3) + \n",
    "(1-\\text{sigmoid}(\\theta_0 * (x - \\theta_1))) (\\theta_4 x + \\theta_5) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(-10, 10, 100)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "px.scatter(x = xtest, y =  sigmoid(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lineA = SimpleLinearModel()\n",
    "        self.lineB = SimpleLinearModel()\n",
    "        self.split = nn.Parameter(torch.tensor([0.0]))\n",
    "        self.scale = nn.Parameter(torch.tensor([10.0]))\n",
    "    def forward(self, x):\n",
    "        ind = torch.sigmoid((x-self.split) * self.scale)\n",
    "        return ind * self.lineA(x) + (1-ind)*self.lineB(x)\n",
    "    def numpy_parameters(self):\n",
    "        \"\"\"Return a numpy version of the parameters.\"\"\"\n",
    "        return np.array([p.detach().numpy() for p in self.parameters()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm = PiecewiseModel()\n",
    "values = adam_sgd(pwm, F.l1_loss, dataset, lr=0.1, nepochs=100, batch_size=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, pwm(tx)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(x.min() - 1, x.max() + 1, 1000)\n",
    "y_hat = pwm(torch.from_numpy(xtest)).detach().numpy().flatten()\n",
    "\n",
    "pwm_line = go.Scatter(x=xtest, y=y_hat, name=\"Piecewise Linear\")\n",
    "fig = go.Figure([raw_data_plot, analytic_mse_line, pwm_line])\n",
    "fig.update_yaxes(title_text=\"Y\")\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Linear Combination of Sine Functions\n",
    "\n",
    "$$\n",
    "\\text{SineBasis}_{w, \\phi}(x) = \\sum_{i = 1}^p w_i \\sin\\left(2\\pi i x + \\phi_i\\right)\n",
    "$$\n",
    "\n",
    "### Our Crazy Model\n",
    "\n",
    "$$\n",
    "\\text{SineBasis}_{w, \\phi, u}(x) = \\text{PolyNomial}_{u}(x) + \\text{SineBasis}_{w,\\phi}(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineBasis(nn.Module):\n",
    "    def __init__(self, p=1):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(p,1).double())\n",
    "        self.phase = nn.Parameter(torch.randn(p,1).double())\n",
    "        self.freq = torch.tensor(2*np.pi *(1.0 + np.arange(p))).unsqueeze(0)\n",
    "    def forward(self, x):\n",
    "        w = self.w\n",
    "        phase = self.phase\n",
    "        freq = self.freq\n",
    "        return (torch.sin(x.unsqueeze(1) @ freq + phase.T) @ w).flatten()\n",
    "                         \n",
    "class CrazyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.polym = PolynomialModel(p=2)\n",
    "        self.sinebasis = SineBasis(50)\n",
    "    def forward(self, x):\n",
    "        return self.polym(x) + self.sinebasis(x)\n",
    "    def numpy_parameters(self):\n",
    "        \"\"\"Return a numpy version of the parameters.\"\"\"\n",
    "        return np.array([p.detach().numpy() for p in self.parameters()]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = SineBasis(3)\n",
    "sb(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_model = CrazyModel()\n",
    "values = adam_sgd(crazy_model, F.l1_loss, dataset, lr=0.01, nepochs=50, batch_size=10)\n",
    "print(\"Loss =\", F.l1_loss(ty, crazy_model(tx)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(x.min() - 1, x.max() + 1, 1000)\n",
    "y_hat = crazy_model(torch.from_numpy(xtest)).detach().numpy().flatten()\n",
    "\n",
    "pwm_line = go.Scatter(x=xtest, y=y_hat, name=\"Piecewise Linear\")\n",
    "fig = go.Figure([raw_data_plot, analytic_mse_line, pwm_line])\n",
    "fig.update_yaxes(title_text=\"Y\")\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
