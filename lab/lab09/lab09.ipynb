{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 9: Climate data, CO2 and `xarray`\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due on **October 25th, 2022, at 11:59PM PDT**.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "This lab accompanies a lecture for Berkeley's Data 100 that covers the fundamental physical mechanisms behind global warming and analyzes CO2 and ocean temperature data.\n",
    "\n",
    "Authors: Fernando PÃ©rez and [Dr. Chelle Gentemann](https://cgentemann.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Small style adjustments for more readable plots\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Mauna Loa CO2 data\n",
    "\n",
    "We start by loading the same dataset we used during the lecture, containing CO2 measurements in Mauna Loa, Hawaii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to load data either from the shared directory that has the complete set,\n",
    "# or from the local small copy available in the repo\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "\n",
    "# If you want to run this on the Berkeley data hub, where we have a larger version of the data,\n",
    "# uncomment the below. But do not do that for submitting the Lab, as the version that will\n",
    "# run on the grader needs to use the path above and only has enough data to grade the Lab.\n",
    "#DATA_DIR = Path.home()/Path('shared/climate-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start by loading the CO2 data from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_file = DATA_DIR / \"monthly_in_situ_co2_mlo_cleaned.csv\"\n",
    "data = pd.read_csv(co2_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of missing values that are set to -99.99 and then drop from the dataset\n",
    "data = pd.read_csv(co2_file, na_values=-99.99).dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did this in lecture, as a reminder this is what the data looks like:\n",
    "plt.plot(\"fraction_date\", \"c02\", data=data)\n",
    "plt.plot(\"fraction_date\", \"data_adjusted_seasonally_fit\", data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the annual anomaly\n",
    "\n",
    "Notice how in the plot, there seems to be oscilliations within each year (changing by the months/seasons), but the overall trend of these oscillations appears to be going upward. We are going to try to understand the annual variability on top of the underlying growing trend, and see whether that variability within a given year is itself changing over time or not.\n",
    "\n",
    "The figure above shows an annual cycle, alongside with perhaps some variability in it. As we saw in lecture (recall the super computer movie in talk with cities release of co2 strongest in winter), plants take up CO2 in (northern) spring/summer then release in fall/winter --- so the release is getting stronger. \n",
    "\n",
    "The annual cycles look a bit like waves, and recall that the amplitude of a wave is the height of its peak. Let's try to estimate the increase in amplitude of annual cycle over the years.\n",
    "\n",
    "<!-- why would using a rolling mean make it look like the cycle is getting larger - both summer/winter rather than just winter? with this data can you figure out if summer or winter causing increase? -->\n",
    "\n",
    "In the lecture notebook, we created the following figure, based on a quick and simple groupby operation and removal of the annual mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the annual cycle using groupby\n",
    "annual = data.groupby(data.month).mean()\n",
    "# calculate the anomaly\n",
    "anomaly = annual - annual.mean()\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\"fraction_date\", \"data_filled\", \"r.\", data=data)\n",
    "ax.plot(\"fraction_date\", \"data_adjusted_seasonally_fit\", data=data)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"CO2 fraction in dry air (ppm)\")\n",
    "ax.set_title(\"Monthly Mean CO2\")\n",
    "ax.grid(False)\n",
    "\n",
    "axin1 = ax.inset_axes([0.1, 0.5, 0.35, 0.35])\n",
    "axin1.plot(anomaly.c02, \"b\")\n",
    "axin1.plot(anomaly.c02, \"r.\")\n",
    "axin1.set_title(\"Seasonal Anomaly\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you look closely, that figure isn't quite the same as the one shown in slide 9 of the lecture:\n",
    "\n",
    "<img src=\"annual-anomaly-orig.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by trying to understand the monthly data. The following shows us the data for all the years, by month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter('month', 'c02', data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately it's hard to see what's actually going on here.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Recreate the following figure, that shows the monthly cycle for all the years in the dataset:\n",
    "\n",
    "<img src=\"monthly-cycles-co2.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use `sns.lineplot(...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Next, in order to attempt to recreate the figure in the talk, we're going to find the monthly anomaly averaging across year. To find this anomaly, we will first be \"detrending\" our data: we will be taking our data for each year, and subtracting off the mean from that year to get a sense of the variability about the average for each year. After detrending, we'll average the monthly data across all years. In this question, you will be writing code to detrend the data. You should end up with the following data frame after writing your function and running the cell (only the first few months are shown, it should have data for all 12 months):\n",
    "\n",
    "<img src=\"monthly-co2-anomaly-df.png\" width=\"150px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detrend(f):\n",
    "    detrended_xarr = ...\n",
    "    ...\n",
    "    return detrended_xarr\n",
    "\n",
    "dy = data.groupby('year')\n",
    "c02anomaly = dy.apply(detrend)\n",
    "display(c02anomaly)\n",
    "monthly_anomaly = c02anomaly.groupby('month').mean()[['c02']]\n",
    "monthly_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Next, recreate the following figure, which is much closer to the one in the lecture:\n",
    "\n",
    "<img src=\"annual-anomaly-new.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:** \n",
    "- in order to get that smooth curve, you'll need to use `from scipy.interpolate import CubicSpline`. You can find the documentation for `CubicSpline` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html). Cubic splines are a very useful method to interpolate data between points (i.e., to draw a function connecting multiple points so we get a better idea of what the function may look like in between points). As an extra hint on using CubicSpline, it's implementation is quite similar to any sci-kit learn function you've already used.\n",
    "- The \"Seasonal Anomaly\" inset graph can be created with `ax.inset_axes(...)`. You can treat the `Axes` object returned like any other `Axes` object.\n",
    "- Remember that you can set the tick labels with `ax.set_xticks(...)`.\n",
    "- To get the data points plotted as red dots, when you use `ax.plot(...)`, pass in `\"r.\"` as the argument immediately following any data you pass in. The `\"r\"` indicates the color, while the `\".\"` indicates how the data will be plotted (if you don't put the `\".\"`, it will be a line instead).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Exploring Snow with ERA 5\n",
    "\n",
    "Now, we turn our attention to the global ERA 5 dataset and explore some other questions - it's a very rich and interesting dataset, and the lecture only scratched its surfac!\n",
    "\n",
    "We are going today to focus on just one more bit: we'll take a look at the snow accumulation data for the northern and southern hemispheres.\n",
    "\n",
    "But you should see this as an invitation to keep learning from these data! Think of looking at other variables in the dataset. Is there annual cycle? trend? Some of the data might look very different than the air temperature - eg. precipitation which is either 0 or +. Can you use PDFs to look at changes in distributions over a region? at a point? Or talk about the data a little & what you understand it is measuring? Are any of the data variables related to each other? Can you plot correlations between data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "monthly_2deg_path = DATA_DIR / \"era5_snow_monthly_2deg_aws_v20210920.nc\"\n",
    "\n",
    "# The Berkeley hub has a version with not only snow data but many more variables.\n",
    "# You can explore the full dataset by uncommenting the following if you are on the\n",
    "# campus hub. Do NOT do that for your lab submission, as that data is NOT available\n",
    "# on Gradescope.\n",
    "\n",
    "# Use this version only when running on the hub:\n",
    "# monthly_2deg_path = Path(\"~/shared/climate-data\").expanduser() / \"era5_monthly_2deg_aws_v20210920.nc\"\n",
    "\n",
    "ds = xr.open_dataset(monthly_2deg_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a standard `pandas` DataFrame, where each observation has an integer index and each column contains variable information about that observation. For physical data, we often have data that is collected at points at a particular location on Earth at a particular time. We can think of this data as being indexed by three-dimensions: longitude, latitude, and time. This part of the lab uses a package called `xarray`, a package with a very similar functionality as `pandas`, but is much more versatile in that it allows a user to work easily with such data that may have multidimensional indexing.\n",
    "\n",
    "What we just made above was called an `xarray.Dataset` object, a collection of a bunch of `xarray.DataArray` objects. Intuitively, a single `xarray.DataArray` object contains measurements of a single variable indexed across different locations in space and time, and multiple `xarray.DataArray` objects for different variables can be grouped into a single `xarray.Dataset` for easier bookkeeping/organization.\n",
    "\n",
    "`xarray` also has a really neat interactive feature when you print out a `Dataset` or a `DataArray`: you can click to see the different Dimensions of the dataset, the Coordinates that data points are collected at (in space, given by latitude/longitude combinations, and time), and variable information collected at each point in space and time (Data variables). \n",
    "\n",
    "Feel free to click around on the above output! When cliking the rightmost button for `time`, `latitude`, and `longitude` coordinates, we see that each of these are actually just NumPy arrays, giving us the latitudes/longitudes as well as times that our observations were collected! The Data variables section always tells you what variables you are collecting information about at each of these latitude-longitude-time coordinates. Here, we are collecting information about snow density, at different points in space across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray` has a rich documentation, so you can always check there if you have questions. However, when in doubt, using a `Pandas` type implementation to get something done might work since the packages are so similar. For example, if I wanted all of the unique longitudes in my dataset, I could just do the following, which spits out a DataArray of my longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine we have snow density measurements across three axes: latitude, longitude, and time. If we take all measurements for a particular time (let's say, year), we would have data on snow measurements for each latitude and longitudes for that particular year, a snapshot in time of the worldwide snow density. However, let's suppose we wish to get the snow density measurements for each latitude and longitude, averaged across all the times in the dataset. We are essentially then aggregating over time, such that each latitude/longitude location now only has one snow density measurement: the average over all the years for that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_map = ds.mean(\"time\")  # takes the mean across all variables in ds\n",
    "mean_map.snow_density.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the `snow_density` information from our `xarray.Dataset` to return a `xarray.DataArray`, in the same way we might take a column from a `pandas.DataFrame`. Once again, the output is interactive, so it's really helpful to understand what exactly is contained in this dataset. Provided below is the snow density measurements are given in the from of numpy arrays stacked on top of each other. \n",
    "\n",
    "Consider for a second why this data structure makes sense: stacked numpy arrays are three-dimensional NumPy arrays, or collections of numbers indexed by three numbers as opposed to just two in a standard array or one as in a list. Snow density measurements are given across three dimensions (time, latitude, longitude), so the numpy array with this information should be three dimensional. \n",
    "\n",
    "Once again, `xarray` tells us what each axis of this numpy array means (time, latitude, and longitude), and also gives us some information in the Attributes section, such as the units `snow_density` is measured in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = ds.snow_density\n",
    "snow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Let's look at the snow density measurements across the globe in February and August, respectively. Februrary is considered the peak of winter in the Northern Hemisphere and the peak of summer in the Southern Hemisphere, and vice versa for August.\n",
    "\n",
    "Recreate the following figure, along with a corresponding one for August 1980. \n",
    "\n",
    "*Hint:* using the `snow` variable above, which is an `xarray.Dataset` object of snow density measurements across time and location, how can you extract all of the measurements for February 1980? for August 1980? See [this page](https://docs.xarray.dev/en/stable/user-guide/indexing.html) from the `xarray` documentation about how to index into these objects (try to avoid the methods where you use integer indexing, the beauty of `xarray` is it's label indexing!) For plotting, try to see if there are any similarities with plotting in `pandas` DataFrames.\n",
    "\n",
    "<img src=\"snow-distribution-1980-02.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> list(monthly_anomaly['c02']) == [-0.6838350970017644, 0.07612096774193428, 0.8742380952380925,\n...                                  2.240269841269833, 2.8641545138888853, 2.299296594982075, \n...                                  0.792918871252205, -1.1950176366843068, -2.851366843033514, \n...                                  -2.8573163082437323, -1.453271604938272, -0.15930335097002282]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
