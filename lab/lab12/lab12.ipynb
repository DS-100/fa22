{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab12.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "# Lab 12: Principal Component Analysis\n",
    "\n",
    "In this lab assignment, we will walk through two examples that use Principal Component Analysis (PCA): one involving a dataset of [iris plants](https://en.wikipedia.org/wiki/Iris_plant), and another involving an artificial \"surfboard\" 3D dataset.\n",
    "\n",
    "\n",
    "### Due Date\n",
    "\n",
    "This assignment is due **Tuesday, November 15th at 11:59 pm PST**.\n",
    "\n",
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** The recording was from Spring 2022, where this lab was labeled Lab 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"gXZ9BJN6zB8\", list = 'PLQCcNQgUcDfqC3hTWqqEErLjHaPq0Yk3A', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight') # Use plt.style.available to see more styles\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "In lecture we discussed how Principal Component Analysis (PCA) can be used for dimensionality reduction. Specifically, given a high dimensional dataset, PCA allows us to:\n",
    "1. Understand the rank of the data. If $k$ principal components capture almost all of the variance, then the data is roughly rank $k$.\n",
    "2. Create 2D scatterplots of the data. Such plots are a rank 2 representation of our data, and allow us to visually identify clusters of similar observations.\n",
    "\n",
    "In this lab, you'll learn how to perform PCA using the `np.linalg` package (Part 1), and you'll also build a geometric intuition of PCA to help you understand its strengths (Part 2). We work with low-dimensional datasets for now to focus on the basics; in the homework, you'll explore how PCA works on a high-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: The Iris Dataset\n",
    "\n",
    "To begin, run the following cell to load the dataset into this notebook. \n",
    "* `iris_features` will contain a numpy array of 4 attributes for 150 different plants (shape `150 x 4`). \n",
    "* `iris_target` will contain the class of each plant. There are 3 classes of plants in the dataset: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. The class names will be stored in `iris_target_names`.\n",
    "* `iris_feature_names` will be a list of 4 names, one for each attribute in `iris_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris() # Loading the dataset\n",
    "\n",
    "# Unpacking the data into arrays\n",
    "iris_features = iris_data['data']\n",
    "iris_target = iris_data['target']\n",
    "iris_feature_names = iris_data['feature_names']\n",
    "iris_target_names = iris_data['target_names']\n",
    "\n",
    "# Convert iris_target to string labels instead of int labels currently (0, 1, 2) for the classes\n",
    "iris_target = iris_target_names[iris_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data by creating a scatter matrix of our iris features. To do this, we'll create 2D scatter plots for every possible pair of our four features. This should result in six total scatter plots in our scatter matrix with the classes labeled in distinct colors for each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"Scatter Matrix of Iris Features\", fontsize=20)\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "for i in range(1, 4):\n",
    "    for j in range(i):\n",
    "        plot_index = 3*j + i\n",
    "        plt.subplot(3, 3, plot_index)\n",
    "        sns.scatterplot(x=iris_features[:, i],\n",
    "                        y=iris_features[:, j],\n",
    "                        hue=iris_target,\n",
    "                       legend=(plot_index == 1))\n",
    "        plt.xlabel(iris_feature_names[i])\n",
    "        plt.ylabel(iris_feature_names[j])\n",
    "        if plot_index == 1:\n",
    "            plt.legend().remove()\n",
    "\n",
    "fig.legend(loc='lower left') # same legend for all subplots\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Standardization and SVD\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "To apply PCA, we will first need to center the data so that the mean of each feature is 0. We will also go further and create **standardized** features, where we scale features such that they are centered with standard deviation 1. (We'll explore simple centered data in Part 2.)\n",
    "\n",
    "Compute the columnwise mean of `iris_features` in the cell below and store it in `iris_mean`, and compute the columnwise standard deviation of `iris_features` and store it in `iris_std`. Each should be a numpy array of 4 means/standard deviations, 1 for each feature. Then, subtract `iris_mean` from `iris_features` and divide by `iris_std`, and finally, save the result in `iris_standardized`.\n",
    "\n",
    "**Hints:** \n",
    "* Use `np.mean` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) or `np.average` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.average.html)) to compute `iris_mean`, and pay attention to the `axis` argument. Same for `np.std` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.std.html)).\n",
    "* If you are confused about how numpy deals with arithmetic operations between arrays of different shapes, see this note about [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for explanations/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_mean = ...\n",
    "iris_std = ...\n",
    "iris_standardized = ...\n",
    "iris_mean, iris_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "As you may recall from lecture, PCA is a specific application of the singular value decomposition (SVD) for matrices. In the following cell, let's use the `np.linalg.svd` function ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)) to compute the SVD of our `iris_standardized` matrix. \n",
    "\n",
    "Store the left singular vectors $U$, singular values $\\Sigma$, and (transposed) right singular vectors $V^T$ in `u`, `s`, and `vt`, respectively. Set the `full_matrices` argument of `np.linalg.svd` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "u.shape, s, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "What can we learn from the singular values in `s`?\n",
    "\n",
    "As discussed in lecture, the total variance of the data is also equal to the sum of the squares of the singular values divided by the number of data points, that is:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{\\sum_{i=1}^d{\\sigma_i^2}}{N} = \\sum_{i=1}^d \\frac{\\sigma_i^2}{N}$$\n",
    "\n",
    "where for data $X$ with $N$ datapoints and $d$ features, $\\sigma_i$ is the singular value corresponding to the $i$-th principal component, and $\\text{Var}(X)$ is the total variance of the data. The right-hand side implies that the expression $\\sigma_i^2/N$ is the amount of variance captured by the $i$-th principal component.\n",
    "\n",
    "Compute the total variance of our data below by summing the square of each singular value in `s` and dividing the result by the total number of data points. Store the result in the variable `iris_total_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_total_variance = ...\n",
    "\n",
    "print(\"iris_total_variance: {:.3f} should approximately equal the sum of the feature variances: {:.3f}\"\n",
    "      .format(iris_total_variance, np.sum(np.var(iris_standardized, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `iris_total_variance` is equal to the sum of the standardized feature variances. Since our features are standardized (i.e., have mean 0 and variance 1), `iris_total_variance` is equal to the number of original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Let's now use only the first two principal components to see what a 2D version of our iris data looks like.\n",
    "\n",
    "First, construct the 2D version of the iris data by multiplying our `iris_standardized` array with the first two right singular vectors in $V$. Because the first two right singular vectors are directions for the first two principal components, this will project the iris data down from a 4D subspace to a 2D subspace.\n",
    "\n",
    "**Hints:**\n",
    "* To matrix-multiply two numpy arrays, use `@` or `np.dot`. In case you're interested, the [matmul documentation](https://numpy.org/devdocs/reference/generated/numpy.matmul.html) contrasts the two methods.\n",
    "* Note that in Question 1b, you computed `vt` (SVD decomposition is $U\\Sigma V^T$). The first two right singular vectors in $V$ will be the two rows of `vt`, [transposed](https://numpy.org/devdocs/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T) to be column vectors instead of row vectors. \n",
    "* Since we want to obtain a 2D version of our iris dataset, the shape of `iris_2d` should be (150, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_2d = ...\n",
    "np.sum(iris_2d[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now, run the cell below to create the scatter plot of our 2D version of the iris data, `iris_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "plt.figure(figsize = (9, 6))\n",
    "plt.title(\"PC2 vs. PC1 for Iris Data\")\n",
    "plt.xlabel(\"Iris PC1\")\n",
    "plt.ylabel(\"Iris PC2\")\n",
    "sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1], hue = iris_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "What do you observe about the plot above? If you were given a point in the subspace defined by PC1 and PC2, how well would you be able to classify the point as one of the three Iris types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "### Question 2c\n",
    "\n",
    "What proportion of the total variance is accounted for when we project the iris data down to two dimensions? Compute this quantity in the cell below by dividing the variance captured by the first two singular values (also known as component scores) in `s` by the `iris_total_variance` you calculated previously. Store the result in `iris_2d_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_2d_variance = ...\n",
    "iris_2d_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variance in the data is explained by the two-dimensional projection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3\n",
    "\n",
    "As a last step, we will create a [scree plot](https://en.wikipedia.org/wiki/Scree_plot) to visualize the weight of each principal component. In the cell below, create a scree plot by creating a line plot of the component scores (variance captured by each principal component) vs. the principal component number (1st, 2nd, 3rd, or 4th). Your graph should match the image below:\n",
    "\n",
    "*Hint:* Be sure to label your axes appropriately! You may find `plt.xticks()` ([documentation](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.xticks.html)) helpful for formatting.\n",
    "\n",
    "<img src=\"images/scree.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your plot here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: PCA on 3D Data\n",
    "\n",
    "**In Part 2, our goal is to see visually how PCA is simply the process of rotating the coordinate axes of our data.**\n",
    "\n",
    "The code below reads in a 3D dataset. We have named the DataFrame `surfboard` because the data resembles a surfboard when plotted in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "surfboard = pd.read_csv(\"data/data3d.csv\")\n",
    "surfboard.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial 1] Visualize the Data\n",
    "\n",
    "The cell below will allow you to view the data as a 3D scatterplot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure.\n",
    "\n",
    "You should see that the data is an ellipsoid that looks roughly like a surfboard or a [hashbrown patty](https://www.google.com/search?q=hashbrown+patty&source=lnms&tbm=isch). That is, it is pretty long in one direction, pretty wide in another direction, and relatively thin along its third dimension. We can think of these as the \"length\", \"width\", and \"thickness\" of the surfboard data.\n",
    "\n",
    "Observe that the surfboard is not aligned with the x/y/z axes.\n",
    "\n",
    "If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "fig = px.scatter_3d(surfboard, \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Data (Colorized)**\n",
    "\n",
    "To give the figure a little more visual pop, the following cell does the same plot, but also assigns a pre-determined color value (that we've arbitrarily chosen) to each point. *These colors do not mean anything important*; they're simply there as a visual aid.\n",
    "\n",
    "You might find it useful to use the `colorize_surfboard_data` method later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell (the colorized version of previous cell)\n",
    "def colorize_surfboard_data(df):\n",
    "    colors = pd.read_csv(\"data/surfboard_colors.csv\", header = None).values\n",
    "    df_copy = df.copy()\n",
    "    df_copy.insert(loc = 3, column = \"color\", value = colors)\n",
    "    return df_copy\n",
    "    \n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard), \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], \n",
    "                    color = \"color\", color_continuous_scale = 'RdBu')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Centering and SVD\n",
    "\n",
    "### Question 4a\n",
    "\n",
    "In Part 1, we standardized the Iris data prior to performing SVD, i.e., we made features zero-mean and unit-variance. In this part, we'll try just **centering** our data so that each feature is zero-mean and variance is unchanged.\n",
    "\n",
    "Compute the columnwise mean of `surfboard` in the cell below, and store the result in `surfboard_mean`. You can choose to make `surfboard_mean` a numpy array or a series, whichever is more convenient for you. Regardless of what data type you use, `surfboard_mean` should have 3 means: 1 for each attribute, with the x coordinate first, then y, then z.\n",
    "\n",
    "Then, subtract `surfboard_mean` from `surfboard`, and save the result in `surfboard_centered`. The order of the columns in `surfboard_centered` should be x, then y, then z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surfboard_mean = ...\n",
    "surfboard_centered = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "In the following cell, compute the singular value decomposition (SVD) of `surfboard_centered` as $U\\Sigma V^T$, and store the left singular vectors $U$, singular values $\\Sigma$, and (transposed) right singular vectors $V^T$ in `u2`, `s2`, and `vt2`, respectively.\n",
    "\n",
    "Your code should be very similar to Part 1, Question 1b up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "u2, s2, vt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c: Total Variance\n",
    "\n",
    "In Part 1 Question 1c, we considered standardized features (each with unit variance), whose total variance was simply the count of features. Now, we'll show that the same relationship holds between singular values `s` and the variance of our (unstandardized) data.\n",
    "\n",
    "In the cell below, compute the total variance as the sum of the squares of the singular values $\\sigma_i$ divided by the number of datapoints $N$. Here's that formula again from Question 1c:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{\\sum_{i=1}^d{\\sigma_i^2}}{N} = \\sum_{i=1}^d \\frac{\\sigma_i^2}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_variance_computed_from_singular_values = ...\n",
    "total_variance_computed_from_singular_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "Your `total_variance_computed_from_singular_values` result should be very close to the total variance of the original `surfboard` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "np.var(surfboard, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance of our dataset is given by the sum of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "total_variance_computed_from_surfboard = sum(np.var(surfboard, axis=0))\n",
    "total_variance_computed_from_surfboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note: The variances are the same for both `surfboard_centered` and `surfboard` (why?), so we show only one to avoid redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4d: Variance Explained by First Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, set `variance_explained_by_1st_pc` to the proportion of the total variance explained by the 1st principal component. Your answer should be a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variance_explained_by_1st_pc = ...\n",
    "variance_explained_by_1st_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We can also create a scree plot that shows the proportion of variance explained by all of our principal components, ordered from most to least. You already constructed a scree plot for the Iris data, so we'll leave the surfboard scree plot for you to do on your own time.\n",
    "\n",
    "Instaed, let's try to visualize why PCA is simply a rotation of the coordinate axes (i.e., features) of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 5: V as a Rotation Matrix\n",
    "\n",
    "In lecture, we saw that the first column of $XV$ contained the first principal component values for each observation, the second column of $XV$ contained the second principal component values for each observation, and so forth.\n",
    "\n",
    "Let's give this matrix a name: $P = XV$ is sometimes known as the \"principal component matrix\".\n",
    "\n",
    "Compute the $P$ matrix for the surfboard dataset and store it in the variable `surfboard_pcs`.\n",
    "\n",
    "Hint: What does $X$ represent here: `surfboard` or `surfboard_centered`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surfboard_pcs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [Tutorial 2] Visualizing the Principal Component Matrix\n",
    "\n",
    "In some sense, we can think of $P$ as an output of the PCA procedure.\n",
    "\n",
    "$P$ is a **rotation** of the data such that the data will now appear \"axis aligned\". Specifically, for a 3d dataset, if we plot PC1, PC2, and PC3 along the x, y, and z axes of our plot, then the greatest amount of variation happens along the x-axis, the second greatest amount along the y-axis, and the smallest amount along the z-axis. \n",
    "\n",
    "To visualize this, run the cell below, which will show our data now projected onto the principal component space. Compare with your original figure (from Tutorial 1 in this part), and observe that the data is exactly the same—only it is now rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "surfboard_pcs = surfboard_pcs.rename(columns = {0: \"pc1\", 1: \"pc2\", 2: \"pc3\"})\n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard_pcs), \n",
    "                    x='pc1', y='pc2', z='pc3', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], \n",
    "                    color = 'color', color_continuous_scale = 'RdBu');\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a 2D scatterplot of our surfboard data as well. Note that the resulting is just the 3D plot as viewed from directly \"overhead\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "# just run this cell\n",
    "sns.scatterplot(data = colorize_surfboard_data(surfboard_pcs), \n",
    "                x = 'pc1', y = 'pc2', hue = \"color\", palette = \"RdBu\", legend = False)\n",
    "plt.xlim(-10, 10);\n",
    "plt.ylim(-10, 10);\n",
    "plt.title(\"Top-Down View of $P$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Part 2 Summary\n",
    "\n",
    "Above, we saw that the principal component matrix $P$ is simply the original data rotated in space so that it appears axis-aligned.\n",
    "\n",
    "Whenever we do a 2D scatter plot of only the first 2 columns of $P$, we are simply looking at the data from \"above\", i.e. so that the 3rd (or higher) PC is invisible to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You finished the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> iris_mean.size == 4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.all(np.isclose(iris_mean, np.array([5.84333333, 3.05733333, 3.758, 1.19933333])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.all(np.isclose(iris_std, np.array([0.82530129, 0.43441097, 1.75940407, 0.75969263])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.all(np.isclose(np.zeros(4), np.mean(iris_standardized, axis = 0))) # make sure data is centered at 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.all(np.isclose(np.ones(4), np.std(iris_standardized, axis = 0))) # make sure data has SD 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> -2.56 < np.sum(iris_standardized[0]) < -2.5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> np.all(np.isclose(s, np.array([20.92306556, 11.7091661 ,  4.69185798,  1.76273239])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> u.shape == (150, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> vt.shape == (4, 4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> np.isclose(iris_total_variance, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.isclose(iris_total_variance, np.sum(np.var(iris_standardized, axis=0)))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> iris_2d.shape == (150, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> -2.75 < np.sum(iris_2d[0]) < -2.74\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> np.isclose(0, np.sum(iris_2d))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> 0.95 < iris_2d_variance < 0.96\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> len(surfboard_mean) == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> abs(surfboard_mean[0] - 0.0278) < 1e-4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> abs(surfboard_mean[1] - 0.0332) < 1e-4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> abs(surfboard_mean[2] - (-0.0203)) < 1e-4\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST \n>>> isinstance(surfboard_centered, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> all(np.isclose(np.mean(surfboard_centered, axis = 0), np.array([0, 0, 0])))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> all(np.isclose([sum(u2[:, i] ** 2) for i in range(u2.shape[1])], [1, 1, 1]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> all(np.isclose([sum(vt2[i, :] ** 2) for i in range(vt2.shape[1])], [1, 1, 1]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> len(s2) == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> (np.isclose(u2 @ np.diag(s2) @ vt2, surfboard_centered)).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> abs(total_variance_computed_from_singular_values - sum(np.var(surfboard, axis=0))) < 1e-6\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> 0.5 < variance_explained_by_1st_pc < 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # TEST\n>>> surfboard_pcs.shape == (1000, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> all(np.isclose(np.mean(surfboard_pcs, axis = 0), [0, 0, 0]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # TEST\n>>> all(np.isclose(abs(surfboard_pcs.loc[0]), [2.648, 0.851, 0.717], atol=1e-3))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
